{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration initiale terminée.\n",
      "Bucket utilisé : m2dsia-dublond-junior-data\n",
      "Dataset BigQuery : dataset_dublond_junior.transactions\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques\n",
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery\n",
    "\n",
    "# Configuration des variables\n",
    "PROJECT_ID = \"isi-groupe-m2-dsia\"\n",
    "BUCKET_NAME = \"m2dsia-dublond-junior-data\"\n",
    "DATASET_ID = \"dataset_dublond_junior\"\n",
    "TABLE_ID = \"transactions\"\n",
    "\n",
    "# Chemins des dossiers\n",
    "input_folder = 'input/'\n",
    "clean_folder = 'clean/'\n",
    "error_folder = 'error/'\n",
    "done_folder = 'done/'\n",
    "\n",
    "# Clients GCP\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# Message de confirmation\n",
    "print(\"Configuration initiale terminée.\")\n",
    "print(f\"Bucket utilisé : {BUCKET_NAME}\")\n",
    "print(f\"Dataset BigQuery : {DATASET_ID}.{TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions utilitaires définies avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour téléverser un fichier dans Cloud Storage\n",
    "def upload_blob(fichier_a_transferer, le_nom_du_fichier_dans_le_cloud):\n",
    "    blob = bucket.blob(le_nom_du_fichier_dans_le_cloud)\n",
    "    blob.upload_from_filename(fichier_a_transferer)\n",
    "    print(f\"Fichier {fichier_a_transferer} téléversé avec succès vers {le_nom_du_fichier_dans_le_cloud}.\")\n",
    "\n",
    "# Fonction pour lister les fichiers dans un dossier\n",
    "def get_blobs_from_bucket(folder):\n",
    "    blobs = bucket.list_blobs(prefix=folder)\n",
    "    print(f\"Liste des fichiers dans {folder}:\")\n",
    "    for blob in blobs:\n",
    "        print(f\" - {blob.name}\")\n",
    "    return bucket.list_blobs(prefix=folder)  # Retourner la liste pour réutilisation\n",
    "\n",
    "# Fonction pour copier un fichier vers un autre dossier\n",
    "def copy_from_blob(blob, destination_folder):\n",
    "    bucket.copy_blob(blob, bucket, destination_folder + '/' + blob.name.split('/')[-1])\n",
    "    print(f\"Fichier {blob.name} copié avec succès vers {destination_folder}.\")\n",
    "\n",
    "# Fonction pour supprimer un fichier\n",
    "def delete_file(filename):\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.delete()\n",
    "    print(f\"Fichier {filename} supprimé avec succès.\")\n",
    "\n",
    "# Message de confirmation\n",
    "print(\"Fonctions utilitaires définies avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction de validation et nettoyage définie avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour valider et nettoyer les données\n",
    "def validate_and_clean_data(file_path):\n",
    "    try:\n",
    "        # Lire le fichier CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Validation des colonnes\n",
    "        required_columns = ['transaction_id', 'product_name', 'category', 'price', 'quantity', 'date', 'customer_name', 'customer_email']\n",
    "        if not all(column in df.columns for column in required_columns):\n",
    "            raise ValueError(\"Colonnes manquantes dans le fichier.\")\n",
    "        \n",
    "        # Nettoyage des données\n",
    "        df['transaction_id'] = df['transaction_id'].astype('int64')\n",
    "        df['product_name'] = df['product_name'].astype('string')\n",
    "        df['category'] = df['category'].astype('string')\n",
    "        df['price'] = df['price'].astype('float64')\n",
    "        df['quantity'] = df['quantity'].astype('int64')\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "        df['customer_name'] = df['customer_name'].astype('string')\n",
    "        df['customer_email'] = df['customer_email'].astype('string')\n",
    "        \n",
    "        print(f\"Fichier {file_path} validé et nettoyé avec succès.\")\n",
    "        return df, None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la validation ou du nettoyage du fichier {file_path}: {e}\")\n",
    "        return None, str(e)\n",
    "\n",
    "# Message de confirmation\n",
    "print(\"Fonction de validation et nettoyage définie avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des fichiers dans input/:\n"
     ]
    }
   ],
   "source": [
    "# Traitement des fichiers dans le dossier input/\n",
    "def process_files():\n",
    "    blobs = get_blobs_from_bucket(input_folder)\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.csv'):\n",
    "            file_name = blob.name.split('/')[-1]\n",
    "            file_path = f\"gs://{BUCKET_NAME}/{blob.name}\"\n",
    "            \n",
    "            # Validation et nettoyage\n",
    "            df, error = validate_and_clean_data(file_path)\n",
    "            if df is not None:\n",
    "                # Déplacer vers clean/\n",
    "                copy_from_blob(blob, clean_folder)\n",
    "                delete_file(blob.name)\n",
    "                print(f\"Fichier {file_name} traité avec succès et déplacé vers clean/.\")\n",
    "            else:\n",
    "                # Déplacer vers error/\n",
    "                copy_from_blob(blob, error_folder)\n",
    "                delete_file(blob.name)\n",
    "                print(f\"Fichier {file_name} contient des erreurs et a été déplacé vers error/.\")\n",
    "\n",
    "# Exécution du traitement\n",
    "process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des fichiers dans clean/:\n",
      " - clean/\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger les données dans BigQuery\n",
    "def load_data_to_bigquery():\n",
    "    blobs = get_blobs_from_bucket(clean_folder)\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.csv'):\n",
    "            file_name = blob.name.split('/')[-1]\n",
    "            file_path = f\"gs://{BUCKET_NAME}/{blob.name}\"\n",
    "            \n",
    "            # Charger le fichier nettoyé dans BigQuery\n",
    "            table_ref = bigquery_client.dataset(DATASET_ID).table(TABLE_ID)\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                source_format=bigquery.SourceFormat.CSV,\n",
    "                skip_leading_rows=1,\n",
    "                autodetect=True,\n",
    "                write_disposition='WRITE_APPEND'\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'rb') as source_file:\n",
    "                    job = bigquery_client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "                    job.result()  # Attendre la fin du job\n",
    "                print(f\"Fichier {file_name} chargé avec succès dans BigQuery.\")\n",
    "                \n",
    "                # Déplacer le fichier vers done/\n",
    "                copy_from_blob(blob, done_folder)\n",
    "                delete_file(blob.name)\n",
    "                print(f\"Fichier {file_name} déplacé avec succès vers done/.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du chargement du fichier {file_name} dans BigQuery: {e}\")\n",
    "\n",
    "# Exécution du chargement\n",
    "load_data_to_bigquery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Traiter les fichiers dans le dossier input/\n",
      "2. Charger les données dans BigQuery\n",
      "Liste des fichiers dans clean/:\n",
      " - clean/\n"
     ]
    }
   ],
   "source": [
    "# Interaction utilisateur\n",
    "def main():\n",
    "    print(\"1. Traiter les fichiers dans le dossier input/\")\n",
    "    print(\"2. Charger les données dans BigQuery\")\n",
    "    choix = input(\"Choisissez une option (1 ou 2) : \")\n",
    "    \n",
    "    if choix == '1':\n",
    "        process_files()\n",
    "    elif choix == '2':\n",
    "        load_data_to_bigquery()\n",
    "    else:\n",
    "        print(\"Option invalide. Veuillez choisir 1 ou 2.\")\n",
    "\n",
    "# Exécution du programme\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
